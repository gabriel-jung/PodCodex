"""
podcodex.core.translate — Translation pipeline for podcast transcripts.

Three translation modes:
    - manual  : user provides the translated JSON directly (e.g. via a LLM UI)
    - ollama  : local LLM via Ollama
    - api     : external API (OpenAI, Anthropic, etc.)

Files produced alongside the audio file (in output_dir):
    {stem}.translated.json   — segments with text + text_trad fields
"""

import json
import os
from pathlib import Path
from typing import Literal

from loguru import logger


# ──────────────────────────────────────────────
# Paths
# ──────────────────────────────────────────────


def _translated_json(audio_path: Path, output_dir: str | Path = "") -> Path:
    root = audio_path.parent / output_dir if output_dir else audio_path.parent
    return root / f"{audio_path.stem}.translated.json"


# ──────────────────────────────────────────────
# Prompt
# ──────────────────────────────────────────────


def _build_system_prompt(
    context: str = "", source_lang: str = "French", target_lang: str = "English"
) -> str:
    context_line = f"Context: {context}" if context else ""
    return f"""You are processing a transcript from a podcast generated by Whisper in {source_lang}.
{context_line}

You must perform two tasks simultaneously:

1. Transcript correction
- Fix automatic transcription errors (nonsensical words, phonetic mistakes)
- Keep the oral and conversational style — do not rephrase or restructure
- **Proper nouns:** names of people, brands, movies, and places are often misheard by Whisper — correct their spelling based on context.
- If a sentence is incomprehensible and unrecoverable, write [inaudible]
- Do NOT correct style, hesitations, or natural repetitions
- NEVER shorten, summarize or omit any part of the original text — every word matters

2. Translation from {source_lang} to {target_lang}
- Translate only the text fields into natural, conversational {target_lang}
- Preserve the oral tone and style of the podcast
- Do not translate proper nouns (people, films, places)
- Translate the full text — never truncate or summarize

Output format:
Return a JSON array with exactly the same elements, adding a text_trad field to each entry.
The original corrected text field must remain present and complete.
Reply ONLY with valid JSON, no surrounding text, no markdown.
Format: [{{"index": 0, "text": "...", "text_trad": "..."}}, ...]"""


def _build_polish_prompt(context: str = "", source_lang: str = "French") -> str:
    context_line = f"Context: {context}" if context else ""
    return f"""You are correcting a transcript from a podcast generated by Whisper in {source_lang}.
{context_line}

Your task: transcript correction only — do NOT translate.
- Fix automatic transcription errors (nonsensical words, phonetic mistakes)
- Keep the oral and conversational style — do not rephrase or restructure
- **Proper nouns:** names of people, brands, movies, and places are often misheard by Whisper — correct their spelling based on context.
- If a sentence is incomprehensible and unrecoverable, write [inaudible]
- Do NOT correct style, hesitations, or natural repetitions
- NEVER shorten, summarize or omit any part of the original text — every word matters

Output format:
Return a JSON array with exactly the same elements, with the corrected text field.
Reply ONLY with valid JSON, no surrounding text, no markdown.
Format: [{{"index": 0, "text": "..."}}]"""


# ──────────────────────────────────────────────
# STEP 2 — Translation
# ──────────────────────────────────────────────


def translate_segments(
    segments: list[dict],
    mode: Literal["manual", "ollama", "api"] = "ollama",
    task: Literal["translate", "polish"] = "translate",
    context: str = "",
    source_lang: str = "French",
    target_lang: str = "English",
    model: str = "",
    api_base_url: str = "https://api.openai.com/v1",
    api_key: str | None = None,
    batch_size: int = 10,
) -> list[dict]:
    """
    Translate transcript segments.

    Modes:
        manual : segments already contain text_trad — validated and returned as-is
        ollama : local LLM via Ollama
        api    : OpenAI-compatible API (OpenAI, Anthropic, Mistral, etc.)

    Args:
        segments    : output of simplify_transcript()
        mode        : translation mode
        task        : "translate" (correct + translate) or "polish" (correct only, no text_trad)
        context     : podcast context to guide the LLM (e.g. "French podcast about film music")
        source_lang : source language (e.g. "French", "Spanish")
        target_lang : target language (e.g. "English", "German")
        model       : LLM model name (ollama: "qwen3:4b", api: "gpt-4o")
        api_base_url: base URL for OpenAI-compatible API
        api_key     : API key — None reads from API_KEY env variable
        batch_size  : number of segments per LLM call

    Returns:
        List of segments with text (corrected) and text_trad fields
    """
    if mode == "manual":
        return _validate_manual(segments, task=task)
    elif mode == "ollama":
        return _translate_ollama(
            segments,
            context=context,
            source_lang=source_lang,
            target_lang=target_lang,
            model=model or "qwen3:4b",
            batch_size=batch_size,
            task=task,
        )
    elif mode == "api":
        return _translate_api(
            segments,
            context=context,
            source_lang=source_lang,
            target_lang=target_lang,
            model=model or "gpt-4o",
            api_base_url=api_base_url,
            api_key=api_key,
            batch_size=batch_size,
            task=task,
        )
    else:
        raise ValueError(
            f"Unknown mode: {mode!r}. Choose from 'manual', 'ollama', 'api'."
        )


def _validate_manual(segments: list[dict], task: str = "translate") -> list[dict]:
    """Validate that all segments have the required fields."""
    if task == "translate":
        missing = [i for i, seg in enumerate(segments) if not seg.get("text_trad")]
        if missing:
            raise ValueError(
                f"Manual mode: missing text_trad on {len(missing)} segments (indices: {missing[:5]})"
            )
        for i, seg in enumerate(segments):
            if len(seg.get("text_trad", "")) < len(seg.get("text", "")) * 0.5:
                logger.warning(
                    f"Segment [{i}] translation looks suspiciously short — check it manually"
                )
    logger.info(f"Manual {task} validated — {len(segments)} segments")
    return segments


def _translate_batch(
    batch: list[dict],
    system_prompt: str,
    call_fn,
    min_length_ratio: float = 0.7,
    task: str = "translate",
) -> list[dict]:
    """
    Translate or polish a single batch of segments using the provided call function.
    call_fn(messages) -> raw response string

    Args:
        min_length_ratio: if corrected text is shorter than this ratio of the
                          original, fall back to the original text and warn.
        task: "translate" or "polish"
    """
    import re

    user_content = "\n\n".join(f"[{i}] {seg['text']}" for i, seg in enumerate(batch))
    if task == "polish":
        user_content += f"\n\nCorrect all {len(batch)} numbered segments above."
    else:
        user_content += f"\n\nTranslate all {len(batch)} numbered segments above."

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_content},
    ]

    raw = call_fn(messages)
    raw = re.sub(r"<think>.*?</think>", "", raw, flags=re.DOTALL).strip()
    raw = re.sub(r"^```(?:json)?|```$", "", raw, flags=re.MULTILINE).strip()

    try:
        parsed = json.loads(raw)
        by_index = {item["index"]: item for item in parsed}
    except Exception as e:
        logger.warning(f"Parse error: {e} — batch will keep original text only")
        by_index = {}

    results = []
    for i, seg in enumerate(batch):
        item = by_index.get(i, {})
        original_text = seg["text"]
        corrected_text = item.get("text", original_text)

        # Guard against truncation — fall back to original if too short
        if len(corrected_text) < len(original_text) * min_length_ratio:
            logger.warning(
                f"Segment [{i}] truncated by LLM "
                f"({len(corrected_text)} vs {len(original_text)} chars) — keeping original"
            )
            corrected_text = original_text

        entry = {**seg, "text": corrected_text}
        if task == "translate":
            entry["text_trad"] = item.get("text_trad", "")
        results.append(entry)
    return results


def _translate_ollama(
    segments: list[dict],
    context: str,
    source_lang: str,
    target_lang: str,
    model: str,
    batch_size: int,
    task: str = "translate",
) -> list[dict]:
    from ollama import Client

    client = Client()
    system_prompt = (
        _build_system_prompt(context, source_lang=source_lang, target_lang=target_lang)
        if task == "translate"
        else _build_polish_prompt(context, source_lang=source_lang)
    )
    results = []

    for i in range(0, len(segments), batch_size):
        batch = segments[i : i + batch_size]
        logger.info(
            f"{'Translating' if task == 'translate' else 'Polishing'} batch {i // batch_size + 1}/{-(-len(segments) // batch_size)} via Ollama ({model})"
        )

        def call_fn(messages):
            response = client.chat(
                model=model,
                messages=messages,
                options={"temperature": 0},
                format="json",
            )
            return response.message.content.strip()

        results.extend(_translate_batch(batch, system_prompt, call_fn, task=task))

    return results


def _translate_api(
    segments: list[dict],
    context: str,
    source_lang: str,
    target_lang: str,
    model: str,
    api_base_url: str,
    api_key: str | None,
    batch_size: int,
    task: str = "translate",
) -> list[dict]:
    from openai import OpenAI

    key = api_key or os.environ.get("API_KEY")
    if not key:
        raise ValueError(
            "No API key found. Set API_KEY in your .env file, or pass api_key=."
        )

    client = OpenAI(api_key=key, base_url=api_base_url)
    system_prompt = (
        _build_system_prompt(context, source_lang=source_lang, target_lang=target_lang)
        if task == "translate"
        else _build_polish_prompt(context, source_lang=source_lang)
    )
    results = []

    for i in range(0, len(segments), batch_size):
        batch = segments[i : i + batch_size]
        logger.info(
            f"{'Translating' if task == 'translate' else 'Polishing'} batch {i // batch_size + 1}/{-(-len(segments) // batch_size)} via API ({model})"
        )

        def call_fn(messages):
            response = client.chat.completions.create(
                model=model, messages=messages, temperature=0
            )
            return response.choices[0].message.content.strip()

        results.extend(_translate_batch(batch, system_prompt, call_fn, task=task))

    return results


# ──────────────────────────────────────────────
# STEP 3 — Persistence
# ──────────────────────────────────────────────


def save_translation(
    audio_path: Path | str,
    segments: list[dict],
    output_dir: str | Path = "",
) -> Path:
    """
    Save translated segments to {stem}.translated.json.

    Returns:
        Path to the saved file.
    """
    audio_path = Path(audio_path)
    out = _translated_json(audio_path, output_dir=output_dir)
    out.parent.mkdir(parents=True, exist_ok=True)
    out.write_text(json.dumps(segments, indent=2, ensure_ascii=False), encoding="utf-8")
    logger.success(f"Translation saved — {len(segments)} segments → {out.name}")
    return out


def load_translation(
    audio_path: Path | str,
    output_dir: str | Path = "",
) -> list[dict]:
    """Load translated segments from {stem}.translated.json."""
    out = _translated_json(Path(audio_path), output_dir=output_dir)
    return json.loads(out.read_text(encoding="utf-8"))


def translation_exists(
    audio_path: Path | str,
    output_dir: str | Path = "",
) -> bool:
    """Check if a translation file exists for the given audio."""
    return _translated_json(Path(audio_path), output_dir=output_dir).exists()


# ──────────────────────────────────────────────
# STEP 4 — Export
# ──────────────────────────────────────────────


def translation_to_text(
    segments: list[dict], lang: Literal["fr", "trad", "both"] = "both"
) -> str:
    """
    Format translated segments as plain readable text for quick review.

    Args:
        segments : output of load_translation()
        lang     : "fr" (original), "trad" (translated), "both" (side by side)
    """
    lines = []
    for seg in segments:
        header = f"[{seg['start']:.3f}s - {seg['end']:.3f}s] {seg['speaker']}"
        if lang == "fr":
            lines.append(f"{header}\n{seg['text']}")
        elif lang == "trad":
            lines.append(f"{header}\n{seg.get('text_trad', '[not translated]')}")
        else:
            lines.append(
                f"{header}\n{seg['text']}\n→ {seg.get('text_trad', '[not translated]')}"
            )
    return "\n\n".join(lines)


# ──────────────────────────────────────────────
# Manual translation helper
# ──────────────────────────────────────────────


def build_manual_prompts_batched(
    segments: list[dict],
    batch_minutes: float = 15.0,
    context: str = "",
    source_lang: str = "French",
    target_lang: str = "English",
    task: str = "translate",
) -> list[tuple[list[dict], str]]:
    """
    Split segments into batches by cumulative audio duration and return one
    prompt per batch.

    Batching by duration rather than segment count ensures each prompt covers
    a roughly consistent amount of speech — important since segment lengths
    vary widely (a few seconds to several minutes).

    Args:
        segments      : output of simplify_transcript()
        batch_minutes : max cumulative duration per batch in minutes (default 15)
        context, source_lang, target_lang, task : passed to build_manual_prompt

    Returns:
        List of (batch_segments, prompt_str) tuples, one per batch.
        batch_segments is kept so the UI can show per-batch duration.
    """
    max_seconds = batch_minutes * 60
    batches: list[list[dict]] = []
    current: list[dict] = []
    current_duration = 0.0

    for seg in segments:
        seg_duration = seg.get("end", 0) - seg.get("start", 0)
        # If adding this segment exceeds the limit AND we already have something,
        # flush the current batch first
        if current and current_duration + seg_duration > max_seconds:
            batches.append(current)
            current = []
            current_duration = 0.0
        current.append(seg)
        current_duration += seg_duration

    if current:
        batches.append(current)

    return [
        (
            batch,
            build_manual_prompt(
                batch,
                context=context,
                source_lang=source_lang,
                target_lang=target_lang,
                task=task,
            ),
        )
        for batch in batches
    ]


def build_manual_prompt(
    segments: list[dict],
    context: str = "",
    source_lang: str = "French",
    target_lang: str = "English",
    task: str = "translate",
) -> str:
    """
    Generate a prompt to paste into a LLM UI for manual translation or polishing.
    The LLM should return a JSON array that can be passed to
    translate_segments(..., mode="manual").
    """
    segments_json = json.dumps(segments, ensure_ascii=False, indent=2)

    if task == "polish":
        system_prompt = _build_polish_prompt(context, source_lang=source_lang)
        instruction = (
            f"Here are the {len(segments)} segments to correct.\n"
            "Return the same JSON array with the corrected text field only. Do not add any other field."
        )
    else:
        system_prompt = _build_system_prompt(
            context, source_lang=source_lang, target_lang=target_lang
        )
        instruction = (
            f"Here are the {len(segments)} segments to translate.\n"
            "Return the same JSON array with text_trad added to each entry. Do not modify any other field."
        )

    return f"""{system_prompt}

{instruction}

{segments_json}"""
